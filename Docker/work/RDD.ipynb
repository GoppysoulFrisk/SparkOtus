{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/24 20:55:04 INFO SparkContext: Running Spark version 3.5.7\n",
      "26/01/24 20:55:04 INFO SparkContext: OS info Linux, 6.8.0-90-generic, amd64\n",
      "26/01/24 20:55:04 INFO SparkContext: Java version 1.8.0_442\n",
      "26/01/24 20:55:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/24 20:55:04 INFO ResourceUtils: ==============================================================\n",
      "26/01/24 20:55:04 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "26/01/24 20:55:04 INFO ResourceUtils: ==============================================================\n",
      "26/01/24 20:55:04 INFO SparkContext: Submitted application: Hello Spark\n",
      "26/01/24 20:55:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "26/01/24 20:55:04 INFO ResourceProfile: Limiting resource is cpu\n",
      "26/01/24 20:55:04 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "26/01/24 20:55:05 INFO SecurityManager: Changing view acls to: jovyan\n",
      "26/01/24 20:55:05 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "26/01/24 20:55:05 INFO SecurityManager: Changing view acls groups to: \n",
      "26/01/24 20:55:05 INFO SecurityManager: Changing modify acls groups to: \n",
      "26/01/24 20:55:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jovyan; groups with view permissions: EMPTY; users with modify permissions: jovyan; groups with modify permissions: EMPTY\n",
      "26/01/24 20:55:05 INFO Utils: Successfully started service 'sparkDriver' on port 40191.\n",
      "26/01/24 20:55:05 INFO SparkEnv: Registering MapOutputTracker\n",
      "26/01/24 20:55:05 INFO SparkEnv: Registering BlockManagerMaster\n",
      "26/01/24 20:55:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "26/01/24 20:55:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "26/01/24 20:55:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "26/01/24 20:55:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1b0dc562-7728-4d34-9d93-608628c38117\n",
      "26/01/24 20:55:05 INFO MemoryStore: MemoryStore started with capacity 342.9 MiB\n",
      "26/01/24 20:55:05 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "26/01/24 20:55:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "26/01/24 20:55:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/24 20:55:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "26/01/24 20:55:06 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...\n",
      "26/01/24 20:55:06 INFO TransportClientFactory: Successfully created connection to master/172.20.0.2:7077 after 60 ms (0 ms spent in bootstraps)\n",
      "26/01/24 20:55:06 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260124205506-0007\n",
      "26/01/24 20:55:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35319.\n",
      "26/01/24 20:55:06 INFO NettyBlockTransferService: Server created on 10eb8e2c569f:35319\n",
      "26/01/24 20:55:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "26/01/24 20:55:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10eb8e2c569f, 35319, None)\n",
      "26/01/24 20:55:06 INFO BlockManagerMasterEndpoint: Registering block manager 10eb8e2c569f:35319 with 342.9 MiB RAM, BlockManagerId(driver, 10eb8e2c569f, 35319, None)\n",
      "26/01/24 20:55:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10eb8e2c569f, 35319, None)\n",
      "26/01/24 20:55:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10eb8e2c569f, 35319, None)\n",
      "26/01/24 20:55:06 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.Timestamp\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.time._\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6ad66e46\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.7`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark._\n",
    "\n",
    "val conf = new SparkConf().setAppName(\"RDD\").setMaster(\"local[*]\").set(\"spark.log.level\", \"WARN\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f60a79-f610-4973-a8e1-391f53ff22ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val tripRaw = sc.textFile(\"data/tripdata.csv\")\n",
    "val taxiRaw = sc.textFile(\"data/taxi_zone_lookup.csv\")\n",
    "\n",
    "val tripHeader = tripRaw.first()\n",
    "val taxiHeader = taxiRaw.first()\n",
    "\n",
    "val trip = tripRaw.filter(_ != tripHeader)\n",
    "val taxi = taxiRaw.filter(_ != taxiHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val tripParsed = trip.map(line => line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50cfc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val tripParsed = trip.map { line =>\n",
    "  val arr = line.split(\",\")\n",
    "  try {\n",
    "    val hour = arr(1).substring(11, 13).toInt\n",
    "    val puLocationId = arr(7).toInt\n",
    "    (puLocationId, hour)\n",
    "  } catch {\n",
    "    case _: Throwable => (-1, -1)\n",
    "  }\n",
    "}.filter { case (pu, h) => pu != -1 && h != -1 }\n",
    "\n",
    "tripParsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aaf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val taxiParsed = taxi.map { line =>\n",
    "  val arr = line.split(\",\")\n",
    "  val locationId = arr(0).toInt\n",
    "  val borough = arr(1)\n",
    "  (locationId, borough)\n",
    "}\n",
    "\n",
    "taxiParsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14659a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val joined = tripParsed.join(taxiParsed)\n",
    "\n",
    "val ordersByBoroughHour = joined\n",
    "  .map { case (_, (hour, borough)) => ((borough, hour), 1) }\n",
    "  .reduceByKey(_ + _)\n",
    "\n",
    "val result = ordersByBoroughHour.map {\n",
    "  case ((borough, hour), count) => s\"$borough,$hour,$count\"\n",
    "}\n",
    "\n",
    "result.saveAsTextFile(\"data/orders_by_borough_hour\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
